{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid World"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MDP:\n",
    "    def __init__(self, trans_prob, reward, gamma):\n",
    "        self.trans_prob = trans_prob\n",
    "        self.reward = reward\n",
    "        self.gamma = gamma\n",
    "        self.num_actions = trans_prob.shape[-1]\n",
    "        self.num_states = trans_prob.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_MDP(x, y, goal_coordinates, rewards, prob, wall_coor):\n",
    "    n = x*y\n",
    "    m = 4\n",
    "    trans_prob = np.zeros((n, n, m))\n",
    "    calculate_up_prob(trans_prob, x, y, prob, wall_coor)\n",
    "    calculate_down_prob(trans_prob, x, y, prob, wall_coor)\n",
    "    calculate_left_prob(trans_prob, x, y, prob, wall_coor)\n",
    "    calculate_right_prob(trans_prob, x, y, prob, wall_coor)\n",
    "    \n",
    "    wall_index = wall_coor[1]*x + wall_coor[0]  \n",
    "    for i in range(m):\n",
    "        for j in range(n):\n",
    "            trans_prob[wall_index, j, i] = 0\n",
    "        for j in range(n):\n",
    "            trans_prob[j, wall_index, i] = 0\n",
    "            \n",
    "    reward = np.full((n, n, m), rewards['general'])\n",
    "    pos_goal_state_index = (goal_coordinates['positive'][1]*x + \n",
    "                            goal_coordinates['positive'][0])\n",
    "    neg_goal_state_index = (goal_coordinates['negative'][1]*x + \n",
    "                            goal_coordinates['negative'][0])\n",
    "    reward[pos_goal_state_index, :, :] = rewards['positive']\n",
    "    reward[neg_goal_state_index, :, :] = rewards['negative']\n",
    "\n",
    "    return trans_prob, reward\n",
    "    \n",
    "    \n",
    "def calculate_up_prob(trans_prob, x, y, prob, wall_coor):\n",
    "    for i in range(trans_prob.shape[0]):\n",
    "        state_coor = (i%x, i//x)\n",
    "        \n",
    "        if state_coor[1] == 0:\n",
    "            trans_prob[i, i, 0] = prob['success_prob']\n",
    "        elif (state_coor[0] == wall_coor[0] and state_coor[1] == wall_coor[1] + 1):\n",
    "            trans_prob[i, i, 0] = prob['success_prob']\n",
    "        else:\n",
    "            trans_prob[i, i-x, 0] = prob[\"success_prob\"]\n",
    "        \n",
    "        if state_coor[0] == 0:\n",
    "            trans_prob[i, i, 0] += prob['left_prob']\n",
    "        elif (state_coor[0] == wall_coor[0] + 1 and state_coor[1] == wall_coor[1]):\n",
    "            trans_prob[i, i, 0] += prob['left_prob']\n",
    "        else:\n",
    "            trans_prob[i, i-1, 0] = prob['left_prob']\n",
    "            \n",
    "        if state_coor[0] == x-1:\n",
    "            trans_prob[i, i, 0] += prob['right_prob']\n",
    "        elif (state_coor[0] == wall_coor[0] - 1 and state_coor[1] == wall_coor[1]):\n",
    "            trans_prob[i, i, 0] += prob['right_prob']\n",
    "        else:\n",
    "            trans_prob[i, i+1, 0] = prob['right_prob']\n",
    "            \n",
    "def calculate_down_prob(trans_prob, x, y, prob, wall_coor):\n",
    "    for i in range(trans_prob.shape[0]):\n",
    "        state_coor = (i%x, i//x)\n",
    "        \n",
    "        if state_coor[1] == y-1:\n",
    "            trans_prob[i, i, 1] = prob['success_prob']\n",
    "        elif (state_coor[0] == wall_coor[0] and state_coor[1] == wall_coor[1] - 1):\n",
    "            trans_prob[i, i, 1] = prob['success_prob']\n",
    "        else:\n",
    "            trans_prob[i, i+x, 1] = prob[\"success_prob\"]\n",
    "            \n",
    "        if state_coor[0] == 0:\n",
    "            trans_prob[i, i, 1] += prob['left_prob']\n",
    "        elif (state_coor[0] == wall_coor[0] + 1 and state_coor[1] == wall_coor[1]):\n",
    "            trans_prob[i, i, 1] += prob['left_prob']\n",
    "        else:\n",
    "            trans_prob[i, i-1, 1] = prob['left_prob']\n",
    "            \n",
    "        if state_coor[0] == x-1:\n",
    "            trans_prob[i, i, 1] += prob['right_prob']\n",
    "        elif (state_coor[0] == wall_coor[0] - 1 and state_coor[1] == wall_coor[1]):\n",
    "            trans_prob[i, i, 1] += prob['right_prob']\n",
    "        else:\n",
    "            trans_prob[i, i+1, 1] = prob['right_prob']\n",
    "        \n",
    "def calculate_left_prob(trans_prob, x, y, prob, wall_coor):\n",
    "    for i in range(trans_prob.shape[0]):\n",
    "        state_coor = (i%x, i//x)\n",
    "        \n",
    "        if state_coor[0] == 0:\n",
    "            trans_prob[i, i, 2] = prob['success_prob']\n",
    "        elif (state_coor[0] == wall_coor[0] + 1 and state_coor[1] == wall_coor[1]):\n",
    "            trans_prob[i, i, 2] = prob['success_prob']\n",
    "        else:\n",
    "            trans_prob[i, i-1, 2] = prob[\"success_prob\"]\n",
    "            \n",
    "        if state_coor[1] == y-1:\n",
    "            trans_prob[i, i, 2] += prob['left_prob']\n",
    "        elif state_coor[0] == wall_coor[0] and state_coor[1] == wall_coor[1] - 1:\n",
    "            trans_prob[i, i, 2] += prob['left_prob']\n",
    "        else:\n",
    "            trans_prob[i, i+x, 2] = prob['left_prob']\n",
    "            \n",
    "        if state_coor[1] == 0:\n",
    "            trans_prob[i, i, 2] += prob['right_prob']\n",
    "        elif state_coor[0] == wall_coor[0] and state_coor[1] == wall_coor[1] + 1:\n",
    "            trans_prob[i, i, 2] += prob['right_prob']\n",
    "        else:\n",
    "            trans_prob[i, i-x, 2] = prob['right_prob']\n",
    "            \n",
    "def calculate_right_prob(trans_prob, x, y, prob, wall_coor):\n",
    "    for i in range(trans_prob.shape[0]):\n",
    "        state_coor = (i%x, i//x)\n",
    "        \n",
    "        if state_coor[0] == x-1:\n",
    "            trans_prob[i, i, 3] = prob['success_prob']\n",
    "        elif state_coor[0] == wall_coor[0] - 1 and state_coor[1] == wall_coor[1]:\n",
    "            trans_prob[i, i, 3] = prob['success_prob']\n",
    "        else:\n",
    "            trans_prob[i, i+1, 3] = prob[\"success_prob\"]\n",
    "            \n",
    "        if state_coor[1] == 0:\n",
    "            trans_prob[i, i, 3] += prob['left_prob']\n",
    "        elif state_coor[0] == wall_coor[0] and state_coor[1] == wall_coor[1] + 1:\n",
    "            trans_prob[i, i, 3] += prob['left_prob']\n",
    "        else:\n",
    "            trans_prob[i, i-x, 3] = prob['left_prob']\n",
    "            \n",
    "        if state_coor[1] == y-1:\n",
    "            trans_prob[i, i, 3] += prob['right_prob']\n",
    "        elif state_coor[0] == wall_coor[0] and state_coor[1] == wall_coor[1] - 1:\n",
    "            trans_prob[i, i, 3] += prob['right_prob']\n",
    "        else:\n",
    "            trans_prob[i, i+x, 3] = prob['right_prob']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyIteration:\n",
    "    '''\n",
    "        This class implements the Policy Iteration algorithm for finding\n",
    "        the optimal policy in an MDP.\n",
    "    '''\n",
    "    def __init__(self, mdp : MDP, eps : float) -> None:\n",
    "        '''\n",
    "            Arguments:\n",
    "            ---------\n",
    "                - mdp (An instance of MDP class): MDP for which the\n",
    "                    optimal policy is to be found.\n",
    "                - eps (float): The policy evaluation algorithm stops\n",
    "                    when the difference between value function of two\n",
    "                    consecutive iteration is less than eps.\n",
    "        '''\n",
    "        self.mdp = mdp\n",
    "        self.eps = eps\n",
    "        self.history = {'policy': [], 'val_func': []}\n",
    "\n",
    "    def run(self) -> None:\n",
    "        '''\n",
    "            This method implements the Policy Iteration Algorithm.\n",
    "        '''\n",
    "\n",
    "        policy = np.ones((self.mdp.num_states, 1), dtype=np.int32)\n",
    "        policy_prime = np.zeros((self.mdp.num_states, 1), dtype=np.int32)\n",
    "        # policy_prime = np.full((self.mdp.num_states, 1), 5, dtype=np.int32)\n",
    "        self.num_iters = 0\n",
    "        count = 0\n",
    "\n",
    "        while np.any(policy != policy_prime):\n",
    "            print(count)\n",
    "            count+=1\n",
    "            self.num_iters += 1\n",
    "            policy = policy_prime.copy()\n",
    "\n",
    "            val_func_policy = self._policy_evaluation(policy)\n",
    "            state_act_val_func = self._calc_state_act_val_func(val_func_policy)\n",
    "            policy_prime = self._policy_improvement(state_act_val_func)\n",
    "\n",
    "            self.history['policy'].append(policy)\n",
    "            self.history['val_func'].append(val_func_policy)\n",
    "\n",
    "        self.opt_val_func = val_func_policy\n",
    "        self.opt_policy = policy\n",
    "\n",
    "    def _policy_improvement(self, state_act_val_func : np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "            This method greedily picks the action which has the highest \n",
    "            value of state action value function for each state.\n",
    "\n",
    "            Arguments:\n",
    "            ---------\n",
    "                - state_act_val_func (np.ndarray): A NumPy array of \n",
    "                    shape (num_states, num_actions) which stores the\n",
    "                    value of the state action values.\n",
    "\n",
    "            Returns:\n",
    "            -------\n",
    "                An NumPy array of shape (num_states, 1)\n",
    "        '''\n",
    "        return np.expand_dims(np.argmax(state_act_val_func, axis=1), axis=1)\n",
    "\n",
    "    def _policy_evaluation(self, policy : np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "            This method finds the total expected discounted reward for\n",
    "            a state given the agent follows the policy, i.e. the value\n",
    "            function for each state.\n",
    "\n",
    "            Arguments:\n",
    "            ---------\n",
    "                - policy (np.ndarray): A NumPy array of shape \n",
    "                    (num_states, 1) indicating the action is to be \n",
    "                    picked in a given state.\n",
    "\n",
    "            Returns:\n",
    "            -------\n",
    "                An NumPy array of shape (num_states, 1)\n",
    "        '''\n",
    "\n",
    "        delta = (1-self.mdp.gamma)/self.mdp.gamma\n",
    "        val_func = np.zeros((self.mdp.num_states, 1))\n",
    "        val_func_prime = np.zeros((self.mdp.num_states, 1))\n",
    "\n",
    "        while (delta > self.eps*(1-self.mdp.gamma)/self.mdp.gamma):\n",
    "            delta = 0\n",
    "            trans_prob_policy = self._get_trans_prob_policy(policy)\n",
    "            reward_func_policy = self._get_reward_func_policy(policy)\n",
    "            # print(reward_func_policy)\n",
    "            val_func = val_func_prime.copy()\n",
    "            # print(val_func)\n",
    "\n",
    "            reward = reward_func_policy + self.mdp.gamma*(val_func.T)\n",
    "            # import pdb; pdb.set_trace()\n",
    "            # print(reward)\n",
    "            val_func_prime = np.sum(trans_prob_policy*reward, axis=1, \n",
    "                keepdims=True)\n",
    "\n",
    "            delta = np.amax(np.abs(val_func_prime - val_func))\n",
    "\n",
    "        return val_func_prime\n",
    "\n",
    "    def _get_trans_prob_policy(self, policy : np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "            This method calculates the matrix P_mu (the transition\n",
    "            probability matrix for a given policy) i.e. the transition\n",
    "            probabilities are according to action suggested by the \n",
    "            policy.\n",
    "\n",
    "            Arguments:\n",
    "            --------\n",
    "                - policy (np.ndarray): A NumPy array of shape \n",
    "                    (num_states, 1) indicating the action is to be \n",
    "                    picked in a given state.\n",
    "\n",
    "            Returns:\n",
    "            -------\n",
    "                An NumPy array of shape (num_states, num_states)\n",
    "        '''\n",
    "\n",
    "        trans_prob_policy = np.zeros((self.mdp.num_states, \n",
    "            self.mdp.num_states))\n",
    "\n",
    "        for i in range(self.mdp.num_states):\n",
    "            trans_prob_policy[i, :] = self.mdp.trans_prob[i, :, policy[i, 0]]\n",
    "\n",
    "        return trans_prob_policy\n",
    "\n",
    "    def _get_reward_func_policy(self, policy : np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "            This method calculates the reward function for the actions\n",
    "            suggested by the policy.\n",
    "\n",
    "            Arguments:\n",
    "            ---------\n",
    "                - policy (np.ndarray): A NumPy array of shape \n",
    "                    (num_states, 1) indicating the action is to be \n",
    "                    picked in a given state.\n",
    "\n",
    "            Returns:\n",
    "            -------\n",
    "                An NumPy array of shape (num_states, num_states)\n",
    "\n",
    "        '''\n",
    "        reward_func_policy = np.zeros((self.mdp.num_states, \n",
    "            self.mdp.num_states))\n",
    "\n",
    "        for i in range(self.mdp.num_states):\n",
    "            reward_func_policy[i, :] = self.mdp.reward[i, :, policy[i]]\n",
    "\n",
    "        return reward_func_policy\n",
    "\n",
    "    def _calc_state_act_val_func(self, \n",
    "        val_func_policy : np.ndarray) -> np.ndarray:\n",
    "        '''\n",
    "            This method calculates the state action value function i.e.\n",
    "            the total expected discounted reward that we collect given\n",
    "            a state and an action and then following the policy.\n",
    "\n",
    "            Arguments:\n",
    "            ---------\n",
    "                - val_func_policy (np.ndarray) : An NumPy array of shape\n",
    "                    (num_states, 1) which contains the value function\n",
    "                    for each state.\n",
    "\n",
    "            Returns:\n",
    "            -------\n",
    "                A NumPy array of shape (num_states, num_actions)\n",
    "        '''\n",
    "        state_act_val_func = np.zeros((self.mdp.num_states, \n",
    "            self.mdp.num_actions))\n",
    "\n",
    "        for i in range(self.mdp.num_actions):\n",
    "            reward = (self.mdp.reward[:, :, i] + \n",
    "                self.mdp.gamma*(val_func_policy.T))\n",
    "            state_act_val_func[:, i] = np.sum(\n",
    "                self.mdp.trans_prob[:, :, i]*reward, axis=1)\n",
    "\n",
    "        return state_act_val_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ValueIteration:\n",
    "    '''\n",
    "        This class implements the Value Iteration algorithm for finding\n",
    "        the optimal policy in an MDP.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, mdp : MDP, eps : float) -> None:\n",
    "        '''\n",
    "            Arguments:\n",
    "            ---------\n",
    "                - mdp (An instance of MDP class): MDP for which the\n",
    "                    optimal policy is to be found.\n",
    "                - eps (float): The policy evaluation algorithm stops\n",
    "                    when the difference between value function of two\n",
    "                    consecutive iteration is less than eps.\n",
    "        '''\n",
    "        self.mdp = mdp\n",
    "        self.eps = eps\n",
    "        self.opt_policy = np.full((self.mdp.num_states, 1), -1)\n",
    "\n",
    "    def run(self) -> None:\n",
    "        '''\n",
    "            This method implements the Value Iteration Algorithm.\n",
    "        '''\n",
    "        val_func = np.zeros((self.mdp.num_states, 1))\n",
    "        val_func_prime = np.zeros((self.mdp.num_states, 1))\n",
    "        delta = (1-self.mdp.gamma)/self.mdp.gamma\n",
    "        \n",
    "        count = 0\n",
    "        while (delta > self.eps*(1-self.mdp.gamma)/self.mdp.gamma):\n",
    "            count += 1\n",
    "            print(count)\n",
    "            delta = 0\n",
    "            val_func = val_func_prime.copy()\n",
    "            \n",
    "            for i in range(self.mdp.num_states):\n",
    "                product = np.dot(self.mdp.trans_prob[i, :, :].T, val_func)\n",
    "                exp_imm_reward = np.sum(\n",
    "                    self.mdp.trans_prob[i, :, :]*self.mdp.reward[i, :, :], \n",
    "                    axis=0, \n",
    "                    keepdims=True).T\n",
    "\n",
    "                self.opt_policy[i, 0] = np.argmax(exp_imm_reward + \n",
    "                    self.mdp.gamma*product, axis=0)\n",
    "                val_func_prime[i, 0] = np.amax(exp_imm_reward + \n",
    "                    self.mdp.gamma*product, axis=0)\n",
    "                \n",
    "                if np.abs(val_func_prime[i, 0] - val_func[i, 0]) > delta:\n",
    "                    delta = np.abs(val_func_prime[i, 0] - val_func[i, 0])\n",
    "\n",
    "        self.opt_val_func = val_func_prime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_coordinates = {'positive': (3, 0), 'negative':(3, 1)}\n",
    "rewards = {'general': 0.0,\n",
    "            'positive': 1,\n",
    "            'negative': -100}\n",
    "            \n",
    "prob = {'success_prob': 0.8,\n",
    "        'left_prob': 0.1,\n",
    "        'right_prob': 0.1}\n",
    "        \n",
    "wall_coor = (1, 1)\n",
    "\n",
    "trans_prob, reward = generate_MDP(4, 3, goal_coordinates, rewards, prob, wall_coor)\n",
    "\n",
    "grid_world = MDP(trans_prob, reward, 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "Optimal Policy [[3]\n",
      " [3]\n",
      " [3]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [1]]\n",
      "Optimal Value Function [[  5.46998279]\n",
      " [  6.3130865 ]\n",
      " [  7.18990407]\n",
      " [  8.66890193]\n",
      " [  4.80291171]\n",
      " [  0.        ]\n",
      " [  3.34670351]\n",
      " [-96.67281069]\n",
      " [  4.16148969]\n",
      " [  3.65399095]\n",
      " [  3.22206242]\n",
      " [  1.52624009]]\n"
     ]
    }
   ],
   "source": [
    "pol_iter = PolicyIteration(grid_world, 1e-10)\n",
    "pol_iter.run()\n",
    "print(\"Optimal Policy\", pol_iter.opt_policy)\n",
    "print(\"Optimal Value Function\", pol_iter.opt_val_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "Optimal Policy [[3]\n",
      " [3]\n",
      " [3]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [0]\n",
      " [2]\n",
      " [2]\n",
      " [1]]\n",
      "Optimal Value Function [[  5.46998279]\n",
      " [  6.3130865 ]\n",
      " [  7.18990407]\n",
      " [  8.66890193]\n",
      " [  4.80291171]\n",
      " [  0.        ]\n",
      " [  3.34670351]\n",
      " [-96.67281069]\n",
      " [  4.16148969]\n",
      " [  3.65399095]\n",
      " [  3.22206242]\n",
      " [  1.52624009]]\n"
     ]
    }
   ],
   "source": [
    "val_iter = ValueIteration(grid_world, 1e-10)\n",
    "val_iter.run()\n",
    "print(\"Optimal Policy\", val_iter.opt_policy)\n",
    "print(\"Optimal Value Function\", val_iter.opt_val_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jack's Car Rental Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /home/utkarsh/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/utkarsh/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/utkarsh/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/utkarsh/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/utkarsh/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/utkarsh/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/utkarsh/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/utkarsh/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import poisson\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "# maximum # of cars in each location\n",
    "MAX_CARS = 20\n",
    "\n",
    "# maximum # of cars to move during night\n",
    "MAX_MOVE_OF_CARS = 5\n",
    "\n",
    "# expectation for rental requests in first location\n",
    "RENTAL_REQUEST_FIRST_LOC = 3\n",
    "\n",
    "# expectation for rental requests in second location\n",
    "RENTAL_REQUEST_SECOND_LOC = 4\n",
    "\n",
    "# expectation for # of cars returned in first location\n",
    "RETURNS_FIRST_LOC = 3\n",
    "\n",
    "# expectation for # of cars returned in second location\n",
    "RETURNS_SECOND_LOC = 2\n",
    "\n",
    "DISCOUNT = 0.9\n",
    "\n",
    "# credit earned by a car\n",
    "RENTAL_CREDIT = 10\n",
    "\n",
    "# cost of moving a car\n",
    "MOVE_CAR_COST = 2\n",
    "\n",
    "# all possible actions\n",
    "actions = np.arange(-MAX_MOVE_OF_CARS, MAX_MOVE_OF_CARS + 1)\n",
    "\n",
    "# An up bound for poisson distribution\n",
    "# If n is greater than this value, then the probability of getting n is truncated to 0\n",
    "POISSON_UPPER_BOUND = 11\n",
    "\n",
    "# Probability for poisson distribution\n",
    "# @lam: lambda should be less than 10 for this function\n",
    "poisson_cache = dict()\n",
    "\n",
    "\n",
    "def poisson_probability(n, lam):\n",
    "    global poisson_cache\n",
    "    key = n * 10 + lam\n",
    "    if key not in poisson_cache:\n",
    "        poisson_cache[key] = poisson.pmf(n, lam)\n",
    "    return poisson_cache[key]\n",
    "\n",
    "\n",
    "def expected_return(state, action, state_value, constant_returned_cars):\n",
    "    # initailize total return\n",
    "    returns = 0.0\n",
    "\n",
    "    # cost for moving cars\n",
    "    returns -= MOVE_CAR_COST * abs(action)\n",
    "    # rewards = 0.0\n",
    "\n",
    "    # moving cars\n",
    "    NUM_OF_CARS_FIRST_LOC = min(state[0] - action, MAX_CARS)\n",
    "    NUM_OF_CARS_SECOND_LOC = min(state[1] + action, MAX_CARS)\n",
    "\n",
    "    # go through all possible rental requests\n",
    "    for rental_request_first_loc in range(POISSON_UPPER_BOUND):\n",
    "        for rental_request_second_loc in range(POISSON_UPPER_BOUND):\n",
    "            # probability for current combination of rental requests\n",
    "            prob = poisson_probability(rental_request_first_loc, RENTAL_REQUEST_FIRST_LOC) * \\\n",
    "                poisson_probability(rental_request_second_loc, RENTAL_REQUEST_SECOND_LOC)\n",
    "\n",
    "            num_of_cars_first_loc = NUM_OF_CARS_FIRST_LOC\n",
    "            num_of_cars_second_loc = NUM_OF_CARS_SECOND_LOC\n",
    "\n",
    "            # valid rental requests should be less than actual # of cars\n",
    "            valid_rental_first_loc = min(num_of_cars_first_loc, rental_request_first_loc)\n",
    "            valid_rental_second_loc = min(num_of_cars_second_loc, rental_request_second_loc)\n",
    "\n",
    "            # get credits for renting\n",
    "            reward = (valid_rental_first_loc + valid_rental_second_loc) * RENTAL_CREDIT\n",
    "            num_of_cars_first_loc -= valid_rental_first_loc\n",
    "            num_of_cars_second_loc -= valid_rental_second_loc\n",
    "\n",
    "            if constant_returned_cars:\n",
    "                # get returned cars, those cars can be used for renting tomorrow\n",
    "                returned_cars_first_loc = RETURNS_FIRST_LOC\n",
    "                returned_cars_second_loc = RETURNS_SECOND_LOC\n",
    "                num_of_cars_first_loc = min(num_of_cars_first_loc + returned_cars_first_loc, MAX_CARS)\n",
    "                num_of_cars_second_loc = min(num_of_cars_second_loc + returned_cars_second_loc, MAX_CARS)\n",
    "                returns += prob * (reward + DISCOUNT * state_value[num_of_cars_first_loc, num_of_cars_second_loc])  \n",
    "            else:\n",
    "                for returned_cars_first_loc in range(POISSON_UPPER_BOUND):\n",
    "                    for returned_cars_second_loc in range(POISSON_UPPER_BOUND):\n",
    "                        prob_return = poisson_probability(\n",
    "                            returned_cars_first_loc, RETURNS_FIRST_LOC) * poisson_probability(returned_cars_second_loc, RETURNS_SECOND_LOC)\n",
    "                        num_of_cars_first_loc_ = min(num_of_cars_first_loc + returned_cars_first_loc, MAX_CARS)\n",
    "                        num_of_cars_second_loc_ = min(num_of_cars_second_loc + returned_cars_second_loc, MAX_CARS)\n",
    "                        prob_ = prob_return * prob\n",
    "                        returns += prob_ * (reward + DISCOUNT *\n",
    "                                            state_value[num_of_cars_first_loc_, num_of_cars_second_loc_])\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "def policy_iteration(constant_returned_cars=True):\n",
    "    value = np.zeros((MAX_CARS + 1, MAX_CARS + 1))\n",
    "    policy = np.zeros(value.shape, dtype=np.int)\n",
    "\n",
    "    iterations = 0\n",
    "    while True:\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        fig = sns.heatmap(np.flipud(policy), cmap=\"YlGnBu\")\n",
    "        plt.ylabel('# cars at first location', fontsize=40)\n",
    "        plt.yticks(list(reversed(range(MAX_CARS + 1))), list(range(MAX_CARS + 1)))\n",
    "        plt.xlabel('# cars at second location', fontsize=40)\n",
    "        plt.title('$\\pi_{}$'.format(iterations), fontsize=40)\n",
    "        plt.savefig(f'{iterations}.png')\n",
    "\n",
    "        # policy evaluation (in-place)\n",
    "        while True:\n",
    "            old_value = value.copy()\n",
    "            for i in range(MAX_CARS + 1):\n",
    "                for j in range(MAX_CARS + 1):\n",
    "                    new_state_value = expected_return([i, j], policy[i, j], value, constant_returned_cars)\n",
    "                    value[i, j] = new_state_value\n",
    "            max_value_change = abs(old_value - value).max()\n",
    "            if max_value_change < 1e-4:\n",
    "                break\n",
    "\n",
    "        # policy improvement\n",
    "        policy_stable = True\n",
    "        for i in range(MAX_CARS + 1):\n",
    "            for j in range(MAX_CARS + 1):\n",
    "                old_action = policy[i, j]\n",
    "                action_returns = []\n",
    "                for action in actions:\n",
    "                    if (0 <= action <= i) or (-j <= action <= 0):\n",
    "                        action_returns.append(expected_return([i, j], action, value, constant_returned_cars))\n",
    "                    else:\n",
    "                        action_returns.append(-np.inf)\n",
    "                new_action = actions[np.argmax(action_returns)]\n",
    "                policy[i, j] = new_action\n",
    "                if policy_stable and old_action != new_action:\n",
    "                    policy_stable = False\n",
    "\n",
    "        if policy_stable:\n",
    "            w0, w1 = np.meshgrid(np.arange(MAX_CARS + 1), np.arange(MAX_CARS + 1))\n",
    "\n",
    "            fig = plt.figure(figsize=(15, 10))\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "            ax.plot_surface(w0, w1, value)\n",
    "            plt.xlabel('# cars at second location', fontsize=40)\n",
    "            plt.ylabel('# cars at first location', fontsize=40)\n",
    "            # plt.yticks(list(reversed(range(MAX_CARS + 1))), list(range(MAX_CARS + 1)))\n",
    "            plt.title('v$\\pi_{}$'.format(iterations), fontsize=40)\n",
    "            plt.savefig('Value_function.png')\n",
    "            break\n",
    "\n",
    "        iterations += 1\n",
    "        print(iterations)\n",
    "\n",
    "\n",
    "policy_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/utkarsh/.local/lib/python3.6/site-packages/ipykernel_launcher.py:30: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n"
     ]
    }
   ],
   "source": [
    "def value_iteration(constant_returned_cars=True):\n",
    "    value = np.zeros((MAX_CARS + 1, MAX_CARS + 1))\n",
    "    value_prime = np.zeros_like(value)\n",
    "    policy = np.zeros(value.shape, dtype=np.int)\n",
    "\n",
    "    iterations = 0\n",
    "    while True:\n",
    "        print(iterations)\n",
    "        value = value_prime.copy()\n",
    "        max_change = 0\n",
    "\n",
    "        for i in range(MAX_CARS + 1):\n",
    "            for j in range(MAX_CARS + 1):\n",
    "                action_returns = []\n",
    "\n",
    "                for action in actions:\n",
    "                    if (0 <= action <= i) or (-j <= action <= 0):\n",
    "                        action_returns.append(expected_return([i, j], action, value, constant_returned_cars))\n",
    "                    else:\n",
    "                        action_returns.append(-np.inf)\n",
    "                \n",
    "                new_action = actions[np.argmax(action_returns)]\n",
    "                policy[i, j] = new_action\n",
    "                value_prime[i, j] = np.amax(action_returns)\n",
    "\n",
    "                if np.abs(value[i, j] - value_prime[i, j]) > max_change:\n",
    "                    max_change = np.abs(value[i, j] - value_prime[i, j])\n",
    "\n",
    "        if iterations < 4 or iterations%10 == 0:\n",
    "            plt.figure(figsize=(15, 10))\n",
    "            fig = sns.heatmap(np.flipud(policy), cmap=\"YlGnBu\")\n",
    "            plt.ylabel('# cars at first location', fontsize=40)\n",
    "            plt.yticks(list(reversed(range(MAX_CARS + 1))), list(range(MAX_CARS + 1)))\n",
    "            plt.xlabel('# cars at second location', fontsize=40)\n",
    "            plt.title('$\\pi_{' + str(iterations) + '}', fontsize=40)\n",
    "            plt.savefig(f'{iterations}.png')\n",
    "        \n",
    "        if max_change < 1e-4:\n",
    "            w0, w1 = np.meshgrid(np.arange(MAX_CARS + 1), np.arange(MAX_CARS + 1))\n",
    "            fig = plt.figure(figsize=(15, 10))\n",
    "            ax = fig.add_subplot(111, projection='3d')\n",
    "            ax.plot_surface(w0, w1, value)\n",
    "            plt.xlabel('# cars at second location', fontsize=30)\n",
    "            plt.ylabel('# cars at first location', fontsize=30)\n",
    "            # plt.yticks(list(reversed(range(MAX_CARS + 1))), list(range(MAX_CARS + 1)))\n",
    "            plt.title('v$\\pi_{}$'.format(iterations), fontsize=30)\n",
    "            plt.savefig('Value_function.png')\n",
    "            break\n",
    "\n",
    "        iterations += 1\n",
    "\n",
    "value_iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
